#!/usr/bin/env python3
"""
Enhanced Unified Ray Peat Signal Processor v2.0

Advanced pipeline with:
- Accurate threshold analysis
- Detailed cost estimation and logging
- Resumable processing with checkpoints
- Real-time signal improvement tracking
- Production-ready corpus processing

Author: Aban Hasan
Date: 2025
"""

import os
import json
import time
import logging
import argparse
from pathlib import Path
import pandas as pd
from datetime import datetime, timedelta
import google.generativeai as genai
from typing import Dict, List, Tuple, Optional
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass
import re
from dataclasses import dataclass, asdict
import csv
import hashlib

# Import existing cleaners
import rules_based_cleaners as rbc
import ai_powered_cleaners as aic

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Enhanced constants based on test analysis
MAX_CHUNK_SIZE = 400000  # ~400K characters ‚âà ~100K tokens (reduced for better processing)
MIN_SIGNAL_THRESHOLD = 0.25  # Adjusted based on test data (was 0.3)
AI_IMPROVEMENT_THRESHOLD = 1.5  # 50% improvement needed to use AI result

# Gemini 2.5 Flash Lite pricing (as of 2025)
GEMINI_PRICING = {
    'input_tokens_per_million': 0.10,  # $0.10 per 1M input tokens
    'output_tokens_per_million': 0.40,  # $0.40 per 1M output tokens
    'requests_per_1000': 0.0001  # Minimal request cost
}

@dataclass
class ProcessingResult:
    """Enhanced result tracking with cost analysis."""
    success: bool
    original_size: int
    processed_size: int
    signal_ratio_before: float
    signal_ratio_after: float
    processing_method: str
    key_topics: List[str]
    processing_time: float
    input_tokens: int = 0
    output_tokens: int = 0
    api_calls: int = 0
    estimated_cost: float = 0.0
    error_message: Optional[str] = None
    processed_content: str = ""
    
    @property
    def signal_improvement(self) -> float:
        """Calculate signal improvement ratio."""
        if self.signal_ratio_before == 0:
            return float('inf') if self.signal_ratio_after > 0 else 0
        return self.signal_ratio_after / self.signal_ratio_before
    
    @property
    def compression_ratio(self) -> float:
        """Calculate compression ratio."""
        return self.processed_size / max(self.original_size, 1)

@dataclass
class ProcessingCheckpoint:
    """Checkpoint for resumable processing."""
    last_processed_file: str
    total_files_processed: int
    total_ai_enhanced: int
    total_rules_only: int
    cumulative_cost: float
    cumulative_tokens: int
    start_time: str
    
class EnhancedSignalProcessor:
    """Enhanced processor with cost tracking and resumable processing."""
    
    def __init__(self, api_key: Optional[str] = None, checkpoint_file: Optional[str] = None):
        """Initialize enhanced processor."""
        self.api_key = api_key or os.getenv('GOOGLE_API_KEY') or os.getenv('GEMINI_API_KEY')
        if self.api_key:
            genai.configure(api_key=self.api_key)
            self.ai_model = genai.GenerativeModel('gemini-2.5-flash-lite')
            logger.info("‚úÖ AI enhancement available (Gemini 2.5 Flash Lite)")
        else:
            self.ai_model = None
            logger.warning("‚ö†Ô∏è No API key - AI enhancement disabled")
        
        self.checkpoint_file = checkpoint_file
        self.checkpoint = self._load_checkpoint()
        
        # Enhanced statistics tracking
        self.stats = {
            'start_time': datetime.now(),
            'total_files': 0,
            'rules_only': self.checkpoint.total_rules_only if self.checkpoint else 0,
            'ai_enhanced': self.checkpoint.total_ai_enhanced if self.checkpoint else 0,
            'failed': 0,
            'total_input_size': 0,
            'total_output_size': 0,
            'total_input_tokens': 0,
            'total_output_tokens': 0,
            'total_api_calls': 0,
            'total_estimated_cost': self.checkpoint.cumulative_cost if self.checkpoint else 0.0,
            'signal_improvements': [],
            'processing_times': []
        }
    
    def _load_checkpoint(self) -> Optional[ProcessingCheckpoint]:
        """Load processing checkpoint if exists."""
        if not self.checkpoint_file or not Path(self.checkpoint_file).exists():
            return None
        
        try:
            with open(self.checkpoint_file, 'r') as f:
                data = json.load(f)
            checkpoint = ProcessingCheckpoint(**data)
            logger.info(f"üìç Resuming from checkpoint: {checkpoint.total_files_processed} files processed")
            return checkpoint
        except Exception as e:
            logger.warning(f"Failed to load checkpoint: {e}")
            return None
    
    def _save_checkpoint(self, last_file: str):
        """Save processing checkpoint."""
        if not self.checkpoint_file:
            return
            
        checkpoint = ProcessingCheckpoint(
            last_processed_file=last_file,
            total_files_processed=self.stats['rules_only'] + self.stats['ai_enhanced'],
            total_ai_enhanced=self.stats['ai_enhanced'],
            total_rules_only=self.stats['rules_only'],
            cumulative_cost=self.stats['total_estimated_cost'],
            cumulative_tokens=self.stats['total_input_tokens'] + self.stats['total_output_tokens'],
            start_time=self.stats['start_time'].isoformat()
        )
        
        with open(self.checkpoint_file, 'w') as f:
            json.dump(asdict(checkpoint), f, indent=2)
    
    def estimate_tokens(self, text: str) -> int:
        """Estimate token count (4 chars per token for English)."""
        return len(text) // 4
    
    def calculate_cost(self, input_tokens: int, output_tokens: int, api_calls: int = 1) -> float:
        """Calculate estimated cost for Gemini API usage."""
        input_cost = (input_tokens / 1000000) * GEMINI_PRICING['input_tokens_per_million']
        output_cost = (output_tokens / 1000000) * GEMINI_PRICING['output_tokens_per_million']
        request_cost = (api_calls / 1000) * GEMINI_PRICING['requests_per_1000']
        return input_cost + output_cost + request_cost
    
    def analyze_corpus_thresholds(self, analysis_file: str) -> Dict:
        """Analyze corpus to determine optimal thresholds."""
        if not Path(analysis_file).exists():
            return {}
        
        df = pd.read_csv(analysis_file)
        
        # Analyze quality score distributions
        threshold_analysis = {
            'total_files': len(df),
            'tier1_candidates': 0,
            'tier2_candidates': 0,
            'quality_distributions': {},
            'recommended_thresholds': {}
        }
        
        for col in ['textual_fidelity_score', 'semantic_noise_score', 'document_atomicity_score']:
            if col in df.columns:
                scores = df[col].dropna()
                threshold_analysis['quality_distributions'][col] = {
                    'mean': float(scores.mean()),
                    'median': float(scores.median()),
                    'q25': float(scores.quantile(0.25)),
                    'q75': float(scores.quantile(0.75)),
                    'min': float(scores.min()),
                    'max': float(scores.max())
                }
        
        # Calculate tier distributions based on current thresholds
        for _, row in df.iterrows():
            fidelity = row.get('textual_fidelity_score', 10)
            noise = row.get('semantic_noise_score', 10)
            atomicity = row.get('document_atomicity_score', 10)
            
            if (fidelity >= 4 and noise >= 5 and atomicity >= 5):
                threshold_analysis['tier1_candidates'] += 1
            else:
                threshold_analysis['tier2_candidates'] += 1
        
        # Recommend thresholds for ~70% rules-only, 30% AI-enhanced
        target_tier1_ratio = 0.7
        threshold_analysis['recommended_thresholds'] = {
            'signal_threshold': 0.25,  # Based on test results
            'tier1_ratio': threshold_analysis['tier1_candidates'] / threshold_analysis['total_files'],
            'estimated_ai_files': threshold_analysis['tier2_candidates']
        }
        
        return threshold_analysis
    
    def detect_ray_peat_signal(self, content: str) -> Tuple[float, List[str]]:
        """Enhanced Ray Peat signal detection with expanded keywords."""
        if not content:
            return 0.0, []
        
        # Expanded Ray Peat indicator keywords
        ray_peat_indicators = {
            'hormones': ['thyroid', 'progesterone', 'estrogen', 'cortisol', 'testosterone', 'insulin',
                        'pregnenolone', 'dhea', 'prolactin', 'growth hormone', 'aldosterone'],
            'metabolism': ['mitochondria', 'glucose', 'glycogen', 'metabolism', 'energy', 'atp',
                         'respiratory quotient', 'metabolic rate', 'oxidative metabolism'],
            'nutrition': ['pufa', 'saturated fat', 'fructose', 'sucrose', 'coconut oil', 'milk',
                         'cheese', 'orange juice', 'honey', 'gelatin', 'calcium'],
            'supplements': ['aspirin', 'niacinamide', 'vitamin e', 'pregnenolone', 'cynomel',
                           'cytomel', 'methylene blue', 'caffeine', 'magnesium'],
            'mechanisms': ['serotonin', 'histamine', 'nitric oxide', 'carbon dioxide', 'lactate',
                          'endotoxin', 'stress response', 'inflammation', 'lipid peroxidation'],
            'health_markers': ['temperature', 'pulse', 'blood sugar', 'cholesterol', 'inflammation',
                              'pulse rate', 'basal temperature', 'metabolic rate']
        }
        
        content_lower = content.lower()
        total_indicators = sum(len(category) for category in ray_peat_indicators.values())
        found_indicators = 0
        found_topics = []
        
        for category, terms in ray_peat_indicators.items():
            category_found = False
            for term in terms:
                if term in content_lower:
                    found_indicators += 1
                    if not category_found:
                        found_topics.append(category)
                        category_found = True
        
        # Enhanced attribution patterns
        attribution_patterns = [
            r'\*\*ray peat:?\*\*',
            r'ray peat (says?|explains?|discusses?|believes?)',
            r'dr\.?\s+ray peat',
            r'according to (dr\.?\s+)?ray peat',
            r'peat (explains?|says?|notes?)',
            r'ray peat\'s (work|research|theory)'
        ]
        
        attribution_score = 0
        for pattern in attribution_patterns:
            matches = len(re.findall(pattern, content_lower))
            attribution_score += matches * 0.1
        
        # Calculate enhanced signal ratio
        base_ratio = found_indicators / total_indicators
        signal_ratio = min(base_ratio + attribution_score, 1.0)
        
        return signal_ratio, list(set(found_topics))
    
    def create_mega_chunks(self, content: str) -> List[Tuple[str, int, int]]:
        """Create massive chunks with enhanced break detection."""
        if len(content) <= MAX_CHUNK_SIZE:
            return [(content, 0, len(content))]
        
        chunks = []
        current_pos = 0
        
        while current_pos < len(content):
            chunk_end = min(current_pos + MAX_CHUNK_SIZE, len(content))
            
            if chunk_end < len(content):
                # Enhanced break point detection
                search_start = max(current_pos, chunk_end - MAX_CHUNK_SIZE // 4)
                
                break_patterns = [
                    (r'\n\n=+ [^=]+ =+\n\n', 'section_header', 1.0),
                    (r'\n\n#{1,6} [^\n]+\n\n', 'markdown_header', 0.95),
                    (r'\n\n\*\*RAY PEAT:\*\*', 'ray_peat_speaker', 0.9),
                    (r'\n\n\*\*HOST:\*\*', 'host_speaker', 0.85),
                    (r'\n\n\*\*CALLER:\*\*', 'caller_speaker', 0.8),
                    (r'\n\n[A-Z][A-Z\s]{15,50}\n\n', 'topic_header', 0.7),
                    (r'\n\n\d+\.\s+[A-Z][^\n]{20,}\n\n', 'numbered_section', 0.65),
                    (r'\.\s*\n\n[A-Z][a-z]', 'paragraph_sentence', 0.5),
                    (r'\n\n[A-Z]', 'paragraph_break', 0.3),
                    (r'\n\n', 'simple_break', 0.1)
                ]
                
                best_break = chunk_end
                best_priority = 0
                
                for pattern, break_type, priority in break_patterns:
                    matches = list(re.finditer(pattern, content[search_start:chunk_end]))
                    if matches and priority > best_priority:
                        best_match = matches[-1]  # Use last match
                        candidate_pos = search_start + best_match.start()
                        
                        # Ensure reasonable chunk sizes
                        if candidate_pos > current_pos + 50000:  # Minimum 50K chars
                            best_break = candidate_pos
                            best_priority = priority
                
                chunk_end = best_break
            
            chunk_text = content[current_pos:chunk_end].strip()
            if chunk_text:
                chunks.append((chunk_text, current_pos, chunk_end))
                logger.debug(f"üì¶ Mega-chunk: {len(chunk_text):,} chars (pos {current_pos:,}-{chunk_end:,})")
            
            current_pos = chunk_end
        
        return chunks
    
    def ai_enhance_signal(self, content: str) -> Dict:
        """Enhanced AI signal extraction with cost tracking."""
        if not self.ai_model:
            raise ValueError("AI model not available")
        
        chunks = self.create_mega_chunks(content)
        enhanced_parts = []
        total_input_tokens = 0
        total_output_tokens = 0
        api_calls = 0
        
        # Enhanced prompt for better signal extraction
        ENHANCED_PROMPT = """You are Dr. Ray Peat's most dedicated student, extracting his pure bioenergetic wisdom from this document.

MISSION: Extract MAXIMUM RAY PEAT SIGNAL while removing ALL noise.

REMOVE COMPLETELY:
- All advertisements, sponsorships, and commercial content
- Host introductions, show logistics, and technical difficulties
- Social talk, pleasantries, and off-topic conversations
- Website mentions, contact information, and promotional content
- Caller management, dead air, and filler content
- Repetitive statements and meandering discussions

PRESERVE AND ENHANCE:
- Every Ray Peat explanation, insight, and scientific discussion
- All bioenergetic principles and metabolic mechanisms
- Practical health recommendations and protocols
- Research citations and scientific evidence
- Relevant questions that directly relate to Ray Peat's teachings

FORMAT REQUIREMENTS:
**RAY PEAT:** [His exact words and teachings - preserve completely]
**CONTEXT:** [Only essential questions/setup that directly relate to Ray Peat's response]

QUALITY STANDARDS:
- Preserve complete scientific explanations (never truncate)
- Maintain technical terminology and precise language
- Keep logical flow of bioenergetic concepts
- Include all practical recommendations and dosages
- Preserve research citations and scientific backing

Your goal: Create a compendious, pure-signal educational resource containing ONLY Ray Peat's bioenergetic teachings.

Document to process:
{content}
"""
        
        for i, (chunk, start_pos, end_pos) in enumerate(chunks):
            logger.info(f"ü§ñ AI processing mega-chunk {i+1}/{len(chunks)} ({len(chunk):,} chars)")
            
            try:
                prompt = ENHANCED_PROMPT.format(content=chunk)
                input_tokens = self.estimate_tokens(prompt)
                
                response = self.ai_model.generate_content(
                    prompt,
                    generation_config=genai.types.GenerationConfig(
                        max_output_tokens=32768,  # Increased from 8192 to prevent truncation
                        temperature=0.1,
                        candidate_count=1
                    )
                )
                
                enhanced_content = response.text.strip()
                output_tokens = self.estimate_tokens(enhanced_content)
                
                total_input_tokens += input_tokens
                total_output_tokens += output_tokens
                api_calls += 1
                
                # Quality validation
                if len(enhanced_content) < 200:
                    logger.warning(f"‚ö†Ô∏è Chunk {i+1} produced short output ({len(enhanced_content)} chars)")
                    continue
                
                signal_ratio, topics = self.detect_ray_peat_signal(enhanced_content)
                if signal_ratio < 0.1:
                    logger.warning(f"‚ö†Ô∏è Chunk {i+1} may have lost Ray Peat signal (ratio: {signal_ratio:.3f})")
                
                enhanced_parts.append(enhanced_content)
                logger.info(f"‚úÖ Chunk {i+1}: {signal_ratio:.3f} signal, {len(topics)} topics, {output_tokens:,} tokens")
                
            except Exception as e:
                logger.error(f"‚ùå AI processing failed for chunk {i+1}: {e}")
                api_calls += 1  # Count failed calls too
                continue
        
        if not enhanced_parts:
            raise ValueError("AI enhancement failed - no valid output generated")
        
        # Combine enhanced parts with clear separation
        combined_content = "\n\n" + ("=" * 50 + "\n\n").join(enhanced_parts)
        final_signal_ratio, final_topics = self.detect_ray_peat_signal(combined_content)
        
        # Calculate costs
        estimated_cost = self.calculate_cost(total_input_tokens, total_output_tokens, api_calls)
        
        return {
            'enhanced_content': combined_content,
            'signal_ratio': final_signal_ratio,
            'key_topics': final_topics,
            'chunks_processed': len(chunks),
            'chunks_successful': len(enhanced_parts),
            'input_tokens': total_input_tokens,
            'output_tokens': total_output_tokens,
            'api_calls': api_calls,
            'estimated_cost': estimated_cost
        }
    
    def process_file(self, file_path: Path, tier_info: Optional[Dict] = None) -> ProcessingResult:
        """Enhanced file processing with detailed tracking."""
        start_time = time.time()
        
        try:
            logger.info(f"üìÑ Processing: {file_path.name}")
            
            # Read original content
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                original_content = f.read()
            
            original_size = len(original_content)
            self.stats['total_input_size'] += original_size
            
            # Purely AI-based cleaning
            processing_method = 'ai_enhanced'
            signal_ratio_before, topics_before = self.detect_ray_peat_signal(original_content)
            signal_ratio_after = signal_ratio_before
            final_topics = topics_before
            input_tokens = output_tokens = api_calls = estimated_cost = 0

            final_content = original_content # Initialize final_content with original_content

            if self.ai_model and len(original_content) > 1000:
                try:
                    logger.info("ü§ñ Applying AI enhancement (purely AI-based approach)...")
                    ai_result = self.ai_enhance_signal(original_content)
                    
                    final_content = ai_result['enhanced_content']
                    signal_ratio_after = ai_result['signal_ratio']
                    final_topics = ai_result['key_topics']
                    input_tokens = ai_result['input_tokens']
                    output_tokens = ai_result['output_tokens']
                    api_calls = ai_result['api_calls']
                    estimated_cost = ai_result['estimated_cost']
                    
                    logger.info(f"‚úÖ AI enhancement successful: {signal_ratio_after:.3f} signal ratio")
                        
                except Exception as e:
                    logger.error(f"‚ùå AI enhancement failed: {e}")
                    processing_method = 'failed_ai_fallback_to_original'
                    # final_content remains original_content due to initialization
                    signal_ratio_after = signal_ratio_before
                    final_topics = topics_before
            
            # Update statistics
            self.stats[processing_method] += 1
            processed_size = len(final_content)
            self.stats['total_output_size'] += processed_size
            self.stats['total_input_tokens'] += input_tokens
            self.stats['total_output_tokens'] += output_tokens
            self.stats['total_api_calls'] += api_calls
            self.stats['total_estimated_cost'] += estimated_cost
            
            processing_time = time.time() - start_time
            self.stats['processing_times'].append(processing_time)
            
            # Track signal improvements
            if processing_method == 'ai_enhanced':
                improvement = signal_ratio_after / max(signal_ratio_before, 0.001)
                self.stats['signal_improvements'].append(improvement)
            
            return ProcessingResult(
                success=True,
                original_size=original_size,
                processed_size=processed_size,
                signal_ratio_before=signal_ratio_before,
                signal_ratio_after=signal_ratio_after,
                processing_method=processing_method,
                key_topics=final_topics,
                processing_time=processing_time,
                input_tokens=input_tokens,
                output_tokens=output_tokens,
                api_calls=api_calls,
                estimated_cost=estimated_cost,
                processed_content=final_content
            )
            
        except Exception as e:
            processing_time = time.time() - start_time
            logger.error(f"‚ùå Failed to process {file_path}: {e}")
            self.stats['failed'] += 1
            
            return ProcessingResult(
                success=False,
                original_size=0,
                processed_size=0,
                signal_ratio_before=0.0,
                signal_ratio_after=0.0,
                processing_method='failed',
                key_topics=[],
                processing_time=processing_time,
                error_message=str(e)
            )
    
    def estimate_corpus_processing(self, input_dir: str, analysis_file: Optional[str] = None) -> Dict:
        """Estimate processing time, costs, and requirements for full corpus."""
        input_path = Path(input_dir)
        
        # Find all files
        file_patterns = ['*.txt', '*.html', '*.md', '*.pdf']
        all_files = []
        for pattern in file_patterns:
            all_files.extend(input_path.rglob(pattern))
        
        # Load analysis data
        tier_data = {}
        if analysis_file and Path(analysis_file).exists():
            df = pd.read_csv(analysis_file)
            for _, row in df.iterrows():
                file_path = row['file_path']
                tier_data[file_path] = {
                    'textual_fidelity_score': row.get('textual_fidelity_score', 10),
                    'semantic_noise_score': row.get('semantic_noise_score', 10),
                    'document_atomicity_score': row.get('document_atomicity_score', 10)
                }
        
        # Analyze files and estimate processing
        total_size = 0
        estimated_ai_files = 0
        estimated_tokens = 0
        
        for file_path in all_files[:100]:  # Sample first 100 files
            try:
                size = file_path.stat().st_size
                total_size += size
                
                # Estimate if file needs AI enhancement
                rel_path = str(file_path.relative_to(input_path.parent))
                tier_info = tier_data.get(rel_path)
                
                if tier_info:
                    fidelity = tier_info.get('textual_fidelity_score', 10)
                    noise = tier_info.get('semantic_noise_score', 10)
                    atomicity = tier_info.get('document_atomicity_score', 10)
                    
                    if fidelity < 4 or noise < 5 or atomicity < 5:
                        estimated_ai_files += 1
                        estimated_tokens += self.estimate_tokens(file_path.read_text(errors='ignore'))
                        
            except Exception:
                continue
        
        # Scale estimates to full corpus
        sample_ratio = min(100 / len(all_files), 1.0)
        
        full_corpus_estimates = {
            'total_files': len(all_files),
            'estimated_ai_files': int(estimated_ai_files / sample_ratio),
            'estimated_rules_files': len(all_files) - int(estimated_ai_files / sample_ratio),
            'estimated_total_tokens': int(estimated_tokens / sample_ratio),
            'estimated_processing_time': {
                'rules_only_hours': (len(all_files) * 0.1) / 3600,  # 0.1 sec per file
                'ai_enhanced_hours': (int(estimated_ai_files / sample_ratio) * 30) / 3600,  # 30 sec per AI file
                'total_hours': ((len(all_files) * 0.1) + (int(estimated_ai_files / sample_ratio) * 30)) / 3600
            },
            'estimated_costs': {
                'total_tokens': int(estimated_tokens / sample_ratio),
                'input_cost': (int(estimated_tokens / sample_ratio) / 1000000) * GEMINI_PRICING['input_tokens_per_million'],
                'output_cost': (int(estimated_tokens / sample_ratio * 0.3) / 1000000) * GEMINI_PRICING['output_tokens_per_million'],
                'total_cost': ((int(estimated_tokens / sample_ratio) / 1000000) * GEMINI_PRICING['input_tokens_per_million']) + 
                             ((int(estimated_tokens / sample_ratio * 0.3) / 1000000) * GEMINI_PRICING['output_tokens_per_million'])
            },
            'thresholds_used': {
                'signal_threshold': MIN_SIGNAL_THRESHOLD,
                'ai_improvement_threshold': AI_IMPROVEMENT_THRESHOLD
            }
        }
        
        return full_corpus_estimates
    
    def process_corpus(self, 
                      input_dir: str,
                      output_dir: str,
                      analysis_file: Optional[str] = None,
                      limit: Optional[int] = None) -> Dict:
        """Process entire corpus with enhanced tracking."""
        input_path = Path(input_dir)
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # Load tier analysis
        tier_data = {}
        if analysis_file and Path(analysis_file).exists():
            try:
                df = pd.read_csv(analysis_file)
                for _, row in df.iterrows():
                    file_path = row['file_path']
                    tier_data[file_path] = {
                        'textual_fidelity_score': row.get('textual_fidelity_score', 10),
                        'semantic_noise_score': row.get('semantic_noise_score', 10),
                        'document_atomicity_score': row.get('document_atomicity_score', 10)
                    }
                logger.info(f"üìä Loaded tier analysis for {len(tier_data)} files")
            except Exception as e:
                logger.warning(f"Could not load tier analysis: {e}")
        
        # Find all files to process
        file_patterns = ['*.txt', '*.html', '*.md', '*.pdf']
        all_files = []
        for pattern in file_patterns:
            all_files.extend(input_path.rglob(pattern))
        
        if limit:
            all_files = all_files[:limit]
        
        logger.info(f"üìÅ Found {len(all_files)} files to process")
        self.stats['total_files'] = len(all_files)
        
        # Process files with checkpointing
        results = []
        metadata = {'processing_timestamp': datetime.now().isoformat(), 'files': []}
        
        for i, file_path in enumerate(all_files, 1):
            logger.info(f"üìÑ Processing file {i}/{len(all_files)}: {file_path.name}")
            
            # Get tier info
            rel_path = str(file_path.relative_to(input_path.parent))
            tier_info = tier_data.get(rel_path)
            
            # Process the file
            result = self.process_file(file_path, tier_info)
            results.append(result)
            
            # Save processed content if successful
            if result.success:
                relative_path = file_path.relative_to(input_path)
                output_subdir = output_path / relative_path.parent
                output_subdir.mkdir(parents=True, exist_ok=True)
                output_file = output_subdir / f"{file_path.stem}_processed.txt"
                
                # Get processed content
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    original = f.read()
                
                processed = result.processed_content
                
                with open(output_file, 'w', encoding='utf-8') as f:
                    f.write(result.processed_content)
                
                # Add to metadata
                metadata['files'].append({
                    'original_file': str(file_path),
                    'output_file': str(output_file),
                    'processing_method': result.processing_method,
                    'signal_ratio_before': result.signal_ratio_before,
                    'signal_ratio_after': result.signal_ratio_after,
                    'signal_improvement': result.signal_improvement,
                    'key_topics': result.key_topics,
                    'original_size': result.original_size,
                    'processed_size': result.processed_size,
                    'compression_ratio': result.compression_ratio,
                    'processing_time': result.processing_time,
                    'estimated_cost': result.estimated_cost
                })
            
            # Save checkpoint every 10 files
            if i % 10 == 0:
                self._save_checkpoint(str(file_path))
                logger.info(f"üíæ Checkpoint saved at file {i}")
            
            # Progress update
            if i % 25 == 0:
                avg_cost = self.stats['total_estimated_cost']
                avg_time = sum(self.stats['processing_times']) / len(self.stats['processing_times'])
                eta_hours = (len(all_files) - i) * avg_time / 3600
                logger.info(f"üìà Progress: {i}/{len(all_files)} ({i/len(all_files)*100:.1f}%) | "
                          f"Cost: ${avg_cost:.2f} | ETA: {eta_hours:.1f}h")
        
        # Save final metadata
        metadata_file = output_path / 'processing_metadata.json'
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2)
        
        # Generate final summary
        successful_results = [r for r in results if r.success]
        
        summary = {
            'total_files': len(all_files),
            'successful': len(successful_results),
            'failed': len([r for r in results if not r.success]),
            'rules_only': self.stats['rules_only'],
            'ai_enhanced': self.stats['ai_enhanced'],
            'total_estimated_cost': self.stats['total_estimated_cost'],
            'total_processing_time': time.time() - self.stats['start_time'].timestamp(),
            'average_signal_improvement': sum(r.signal_improvement for r in successful_results if r.signal_improvement != float('inf')) / max(len([r for r in successful_results if r.signal_improvement != float('inf')]), 1)
        }
        
        # Save summary
        summary_file = output_path / 'processing_summary.json'
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2)
        
        return summary

def main():
    """Enhanced main function with estimation and resumable processing."""
    parser = argparse.ArgumentParser(description='Enhanced Ray Peat Signal Processor v2.0')
    parser.add_argument('--input-dir', required=True, help='Input directory with raw files')
    parser.add_argument('--output-dir', required=True, help='Output directory for processed files')
    parser.add_argument('--analysis-file', help='CSV file with tier analysis')
    parser.add_argument('--limit', type=int, help='Limit number of files to process')
    parser.add_argument('--api-key', help='Google API key for AI enhancement')
    parser.add_argument('--estimate-only', action='store_true', help='Only show estimates, do not process')
    parser.add_argument('--resume', help='Resume from checkpoint file')
    parser.add_argument('--checkpoint-interval', type=int, default=10, help='Save checkpoint every N files')
    
    args = parser.parse_args()
    
    # Initialize enhanced processor
    checkpoint_file = args.resume or (Path(args.output_dir) / 'processing_checkpoint.json')
    processor = EnhancedSignalProcessor(api_key=args.api_key, checkpoint_file=checkpoint_file)
    
    # Show corpus estimates
    logger.info("üîç Analyzing corpus and generating estimates...")
    estimates = processor.estimate_corpus_processing(args.input_dir, args.analysis_file)
    
    print("\n" + "="*80)
    print("üìä CORPUS PROCESSING ESTIMATES")
    print("="*80)
    print(f"üìÅ Total files: {estimates['total_files']:,}")
    print(f"üîß Estimated rules-only files: {estimates['estimated_rules_files']:,}")
    print(f"ü§ñ Estimated AI-enhanced files: {estimates['estimated_ai_files']:,}")
    print(f"\n‚è±Ô∏è ESTIMATED PROCESSING TIME:")
    print(f"   Rules-only: {estimates['estimated_processing_time']['rules_only_hours']:.1f} hours")
    print(f"   AI-enhanced: {estimates['estimated_processing_time']['ai_enhanced_hours']:.1f} hours")
    print(f"   Total: {estimates['estimated_processing_time']['total_hours']:.1f} hours")
    print(f"\nüí∞ ESTIMATED COSTS (Gemini 2.5 Flash Lite):")
    print(f"   Total tokens: {estimates['estimated_costs']['total_tokens']:,}")
    print(f"   Input cost: ${estimates['estimated_costs']['input_cost']:.2f}")
    print(f"   Output cost: ${estimates['estimated_costs']['output_cost']:.2f}")
    print(f"   Total cost: ${estimates['estimated_costs']['total_cost']:.2f}")
    print(f"\nüéØ THRESHOLDS:")
    print(f"   Signal threshold: {estimates['thresholds_used']['signal_threshold']}")
    print(f"   AI improvement threshold: {estimates['thresholds_used']['ai_improvement_threshold']}x")
    print("="*80)
    
    if args.estimate_only:
        return
    
    # Proceed with processing
    print(f"\nüöÄ Starting processing in 5 seconds...")
    time.sleep(5)
    
    # Full corpus processing
    summary = processor.process_corpus(
        input_dir=args.input_dir,
        output_dir=args.output_dir,
        analysis_file=args.analysis_file,
        limit=args.limit
    )
    
    print("\nüéâ Processing completed successfully!")
    print(f"üìä Final Results:")
    print(f"  ‚úÖ Files processed: {summary['successful']}")
    print(f"  üîß Rules-only: {summary['rules_only']}")  
    print(f"  ü§ñ AI-enhanced: {summary['ai_enhanced']}")
    print(f"  üí∞ Total cost: ${processor.stats['total_estimated_cost']:.2f}")
    print(f"  ‚è±Ô∏è Processing time: {(time.time() - processor.stats['start_time'].timestamp()) / 3600:.1f} hours")

if __name__ == "__main__":
    main() 